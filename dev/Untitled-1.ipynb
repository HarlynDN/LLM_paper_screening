{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "prompt_template = open('prompt.txt').read()\n",
    "papers = open('papers.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, paper in enumerate(papers):\n",
    "    try:\n",
    "        title, authors = paper.strip().split('\\xa0')\n",
    "    except:\n",
    "        print(f\"Error on line {i+1}: {paper}\")\n",
    "    papers[i] = {'title': title, 'authors': authors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"papers.json\", \"w\") as f:\n",
    "    f.write(json.dumps(papers, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 任务要求\n",
      "给定一份论文题目的清单，你需要依次在网络中搜寻每一篇论文，然后判断该论文的研究方向是否和以下领域中的一个或多个相关：\n",
      "1. 情感分析（sentiment analysis）\n",
      "2. 情绪识别（emotion recognition）\n",
      "3. 论辩挖掘（argumentation mining）\n",
      "4. 情感对话（empathetic conversation）\n",
      "5. 社会情感（social emotion）\n",
      "\n",
      "你需要以json(字典)的格式依次返回对于每一篇论文的判断结果，格式如下：\n",
      "    {\n",
      "        \"id\": \"论文在清单中的索引\",\n",
      "        \"title\": \"论文题目\",\n",
      "        \"url\": \"论文的网址\", // 如果找不到论文，请返回空字符串\"\"\n",
      "        // 如果找到了url，则继续返回下面的字段，否则不需要返回\n",
      "        \"matched_domain\": [\"领域1\", \"领域2\", ...] // 请以列表形式返回上述领域中的一个或多个；使用领域的中文名称，如\"情感分析\"；如果没有匹配的领域，请返回空列表[]\n",
      "        // 如果存在匹配的领域，则继续返回下面的字段，否则不需要返回\n",
      "        \"authors\": [\"作者1\", \"作者2\", ...], // 作者列表\n",
      "        \"affiliation\": [\"机构1\", \"机构2\", ...], // 机构列表\n",
      "        \"summmary\": \"对这篇论文的简要总结\" // 要求尽可能简洁，不超过100字 \n",
      "    }\n",
      "\n",
      "    不同论文的返回结果之间使用逗号+换行符进行分隔，如下所示：\n",
      "    {\n",
      "        \"id\": \"0\",\n",
      "        \"title\": \"Combo of Thinking and Observing for Outside-Knowledge VQA\",\n",
      "        \"url\": \"url of this paper\",\n",
      "        \"matched_domain\": [],\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"1\",\n",
      "        \"title\": \"Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations\",\n",
      "        \"url\": \"url of this paper\",\n",
      "        \"matched_domain\": [\"情感对话\", \"情绪识别\"],\n",
      "        \"authors\": ['Jun Gao', 'Yuhan Liu', 'Haolin Deng', 'Wei Wang', 'Yu Cao', 'Jiachen Du', 'Ruifeng Xu'],\n",
      "        \"affiliation\": [\"Harbin Institute of Technology (Shenzhen), China\", \"Peng Cheng Laboratory, Shenzhen, China\", \"Shenzhen International Graduate School, Tsinghua University\", \"School of Computer Science, The University of Sydney\"],\n",
      "        \"summmary\": \"这篇论文探讨了通过识别对话中的情绪原因来提升生成共情响应的方法。研究者们开发了一种模型，该模型能够分析对话内容，识别情绪触发因素，并据此生成更为贴切和富有同理心的回复，从而改善人机交互体验。\"\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"2\",\n",
      "        \"title\": \"GPT-5 technical report\",\n",
      "        \"url\": \"\",\n",
      "    }\n",
      "    ...\n",
      "\n",
      "论文清单如下：\n",
      "[START OF LIST]\n",
      "{'id': 0, 'title': 'Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences', 'authors': 'Hongyi Liu, Qingyun Wang, Payam Karisani, Heng Ji'}\n",
      "{'id': 1, 'title': 'Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation', 'authors': 'Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang'}\n",
      "{'id': 2, 'title': 'An Interactive Framework for Profiling News Media Sources', 'authors': 'Nikhil Mehta, Dan Goldwasser'}\n",
      "{'id': 3, 'title': 'Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study', 'authors': 'Yinghao Li, Haorui Wang, Chao Zhang'}\n",
      "{'id': 4, 'title': 'TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation', 'authors': 'Taeyang Yun, Hyunkuk Lim, Jeonghwan Lee, Min Song'}\n",
      "[END OF LIST]\n",
      "\n",
      "\n",
      "现在，请阅读论文清单，并从第一篇论文开始按照要求返回结果。\n"
     ]
    }
   ],
   "source": [
    "start_id = 0\n",
    "end_id = 5\n",
    "paper_batch = '\\n'.join(papers[start_id:end_id])\n",
    "prompt = prompt_template.replace('{PAPER_LIST}', paper_batch)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "            query='Combo of Thinking and Observing for Outside-Knowledge VQA',\n",
    "            max_results=5,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "results = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "NOT_FOUND = {'error': 'not found'}\n",
    "\n",
    "def match(res: arxiv.Result, title: str, authors: list = []):\n",
    "    success = len(set(title.split()).intersection(res.title.split())) / len(title.split()) > 0.9 \n",
    "    if authors:\n",
    "        res_authors = set([a.name for a in res.authors])\n",
    "        success = success and len(set(authors).intersection(res_authors)) / len(set(authors)) > 0.9\n",
    "    return success\n",
    "\n",
    "def search(title: str, authors: list = []):\n",
    "    client = arxiv.Client()\n",
    "    query = title + ' ' + ' '.join(authors)\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=10, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    results = client.results(search)\n",
    "    for res in results:\n",
    "        if match(res, title, authors):\n",
    "            return {'title': res.title, 'authors': [a.name for a in res.authors], 'url': res.entry_id, 'abstract': res.summary}\n",
    "    return NOT_FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'not found'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(title='Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"input\": {\n",
      "            \"title\": \"Combo of Thinking and Observing for Outside-Knowledge VQA\",\n",
      "            \"authors\": [\n",
      "                \"Qingyi Si\",\n",
      "                \"Yuchen Mo\",\n",
      "                \"Zheng Lin\",\n",
      "                \"Huishan Ji\",\n",
      "                \"Weiping Wang\"\n",
      "            ],\n",
      "            \"url\": \"http://arxiv.org/abs/2305.06407v1\",\n",
      "            \"abstract\": \"Outside-knowledge visual question answering is a challenging task that\\nrequires both the acquisition and the use of open-ended real-world knowledge.\\nSome existing solutions draw external knowledge into the cross-modality space\\nwhich overlooks the much vaster textual knowledge in natural-language space,\\nwhile others transform the image into a text that further fuses with the\\ntextual knowledge into the natural-language space and completely abandons the\\nuse of visual features. In this paper, we are inspired to constrain the\\ncross-modality space into the same space of natural-language space which makes\\nthe visual features preserved directly, and the model still benefits from the\\nvast knowledge in natural-language space. To this end, we propose a novel\\nframework consisting of a multimodal encoder, a textual encoder and an answer\\ndecoder. Such structure allows us to introduce more types of knowledge\\nincluding explicit and implicit multimodal and textual knowledge. Extensive\\nexperiments validate the superiority of the proposed method which outperforms\\nthe state-of-the-art by 6.17% accuracy. We also conduct comprehensive ablations\\nof each component, and systematically study the roles of varying types of\\nknowledge. Codes and knowledge data can be found at\\nhttps://github.com/PhoebusSi/Thinking-while-Observing.\"\n",
      "        },\n",
      "        \"output\": \"不相关\"\n",
      "    },\n",
      "    {\n",
      "        \"input\": {\n",
      "            \"title\": \"Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations\",\n",
      "            \"authors\": [\n",
      "                \"Jun Gao\",\n",
      "                \"Yuhan Liu\",\n",
      "                \"Haolin Deng\",\n",
      "                \"Wei Wang\",\n",
      "                \"Yu Cao\",\n",
      "                \"Jiachen Du\",\n",
      "                \"Ruifeng Xu\"\n",
      "            ],\n",
      "            \"url\": \"https://aclanthology.org/2021.findings-emnlp.70\",\n",
      "            \"abstract\": \"Current approaches to empathetic response generation focus on learning a model to predict an emotion label and generate a response based on this label and have achieved promising results. However, the emotion cause, an essential factor for empathetic responding, is ignored. The emotion cause is a stimulus for human emotions. Recognizing the emotion cause is helpful to better understand human emotions so as to generate more empathetic responses. To this end, we propose a novel framework that improves empathetic response generation by recognizing emotion cause in conversations. Specifically, an emotion reasoner is designed to predict a context emotion label and a sequence of emotion cause-oriented labels, which indicate whether the word is related to the emotion cause. Then we devise both hard and soft gated attention mechanisms to incorporate the emotion cause into response generation. Experiments show that incorporating emotion cause information improves the performance of the model on both emotion recognition and response generation.\"\n",
      "        },\n",
      "        \"output\": {\n",
      "            \"domains\": [\n",
      "                \"情感对话\",\n",
      "                \"情绪识别\"\n",
      "            ],\n",
      "            \"summary\": \"这篇论文探讨了通过识别对话中的情绪原因来提升生成共情响应的方法。研究者们开发了一种模型，该模型能够分析对话内容，识别情绪触发因素，并据此生成更为贴切和富有同理心的回复，从而改善人机交互体验。\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        {\n",
    "            \"input\": {\n",
    "                \"title\": \"Combo of Thinking and Observing for Outside-Knowledge VQA\",\n",
    "                \"authors\": [\n",
    "                    \"Qingyi Si\",\n",
    "                    \"Yuchen Mo\",\n",
    "                    \"Zheng Lin\",\n",
    "                    \"Huishan Ji\",\n",
    "                    \"Weiping Wang\"\n",
    "                ],\n",
    "                \"url\": \"http://arxiv.org/abs/2305.06407v1\",\n",
    "                \"abstract\": \"Outside-knowledge visual question answering is a challenging task that\\nrequires both the acquisition and the use of open-ended real-world knowledge.\\nSome existing solutions draw external knowledge into the cross-modality space\\nwhich overlooks the much vaster textual knowledge in natural-language space,\\nwhile others transform the image into a text that further fuses with the\\ntextual knowledge into the natural-language space and completely abandons the\\nuse of visual features. In this paper, we are inspired to constrain the\\ncross-modality space into the same space of natural-language space which makes\\nthe visual features preserved directly, and the model still benefits from the\\nvast knowledge in natural-language space. To this end, we propose a novel\\nframework consisting of a multimodal encoder, a textual encoder and an answer\\ndecoder. Such structure allows us to introduce more types of knowledge\\nincluding explicit and implicit multimodal and textual knowledge. Extensive\\nexperiments validate the superiority of the proposed method which outperforms\\nthe state-of-the-art by 6.17% accuracy. We also conduct comprehensive ablations\\nof each component, and systematically study the roles of varying types of\\nknowledge. Codes and knowledge data can be found at\\nhttps://github.com/PhoebusSi/Thinking-while-Observing.\"\n",
    "            },\n",
    "            \"output\": \"不相关\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": {\n",
    "                \"title\": \"Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations\",\n",
    "                \"authors\": [\n",
    "                    \"Jun Gao\",\n",
    "                    \"Yuhan Liu\",\n",
    "                    \"Haolin Deng\",\n",
    "                    \"Wei Wang\",\n",
    "                    \"Yu Cao\",\n",
    "                    \"Jiachen Du\",\n",
    "                    \"Ruifeng Xu\"\n",
    "                ],\n",
    "                \"url\": \"https://aclanthology.org/2021.findings-emnlp.70\",\n",
    "                \"abstract\": \"Current approaches to empathetic response generation focus on learning a model to predict an emotion label and generate a response based on this label and have achieved promising results. However, the emotion cause, an essential factor for empathetic responding, is ignored. The emotion cause is a stimulus for human emotions. Recognizing the emotion cause is helpful to better understand human emotions so as to generate more empathetic responses. To this end, we propose a novel framework that improves empathetic response generation by recognizing emotion cause in conversations. Specifically, an emotion reasoner is designed to predict a context emotion label and a sequence of emotion cause-oriented labels, which indicate whether the word is related to the emotion cause. Then we devise both hard and soft gated attention mechanisms to incorporate the emotion cause into response generation. Experiments show that incorporating emotion cause information improves the performance of the model on both emotion recognition and response generation.\"\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"domains\": [\n",
    "                    \"情感对话\",\n",
    "                    \"情绪识别\"\n",
    "                ],\n",
    "                \"summary\": \"这篇论文探讨了通过识别对话中的情绪原因来提升生成共情响应的方法。研究者们开发了一种模型，该模型能够分析对话内容，识别情绪触发因素，并据此生成更为贴切和富有同理心的回复，从而改善人机交互体验。\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "print(json.dumps(data, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Combo of Thinking and Observing for Outside-Knowledge VQA',\n",
       " 'authors': ['Qingyi Si',\n",
       "  'Yuchen Mo',\n",
       "  'Zheng Lin',\n",
       "  'Huishan Ji',\n",
       "  'Weiping Wang'],\n",
       " 'url': 'http://arxiv.org/abs/2305.06407v1',\n",
       " 'abstract': 'Outside-knowledge visual question answering is a challenging task that\\nrequires both the acquisition and the use of open-ended real-world knowledge.\\nSome existing solutions draw external knowledge into the cross-modality space\\nwhich overlooks the much vaster textual knowledge in natural-language space,\\nwhile others transform the image into a text that further fuses with the\\ntextual knowledge into the natural-language space and completely abandons the\\nuse of visual features. In this paper, we are inspired to constrain the\\ncross-modality space into the same space of natural-language space which makes\\nthe visual features preserved directly, and the model still benefits from the\\nvast knowledge in natural-language space. To this end, we propose a novel\\nframework consisting of a multimodal encoder, a textual encoder and an answer\\ndecoder. Such structure allows us to introduce more types of knowledge\\nincluding explicit and implicit multimodal and textual knowledge. Extensive\\nexperiments validate the superiority of the proposed method which outperforms\\nthe state-of-the-art by 6.17% accuracy. We also conduct comprehensive ablations\\nof each component, and systematically study the roles of varying types of\\nknowledge. Codes and knowledge data can be found at\\nhttps://github.com/PhoebusSi/Thinking-while-Observing.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
